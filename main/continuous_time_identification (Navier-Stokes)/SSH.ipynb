{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations\n",
    "\n",
    "## Author:M. Raissi, P. Perdikaris, G.E. Karniadakis\n",
    "\n",
    "[github](https://github.com/maziarraissi/PINNs)\n",
    "\n",
    "This piece of code is to predict the velocity fields and the pressure of a fluid system's flow status, based on Naiver-Stoke 2D equation,the results are demonstrated in **tensorflow 2 using the SSH data**. As the original code was written in tensorflow 1, variations were made to compat the latest version\n",
    "\n",
    "In this work, we consider parametrized and nonlinear partial differential equations of the general form as\n",
    "\n",
    "$u_t + N[u; λ] = 0, x ∈ \\mathsf{\\Omega}􏰀, t ∈ [0, T]$\n",
    "\n",
    "where $u_t$ denotes the latent solution, $N[u; λ]$ is a nonlinear operator parameterized by λ, a weight variable\n",
    "\n",
    "The current code is for the continuous time model, hence from the above we can define\n",
    "$f :=u_t +N[u]$\n",
    "by proceeding $u(t,x)$ to the neural network, we can get $f(t,x)$ as the result.\n",
    "\n",
    "This network can be derived by applying the chain rule for differentiating compositions of functions using automatic differentiation, and has the same parameters as the network representing $u(t,x)$, albeit with different activation functions due to the action of the differential operator $N$. But both $u(t,x)$ and $f(t,x)$ can be operated by minimizing the mean squared error loss where:\n",
    "$$MSE=MSE_u+MSE_f$$\n",
    "$$MSE_U = \\frac{1}{N_u}\\sum_{i=1}^{N_u}\\lvert u(t_u^i,x_u^i)-u^i \\rvert^2$$\n",
    "$$MSE_f = \\frac{1}{N_f}\\sum_{i=1}^{N_f}\\lvert f(t_f^i,x_f^i) \\rvert^2$$\n",
    "\n",
    "As the $MSE_U$ represent the loss for the first neural net,a multi-layered neural network, with hyperbolic tangent as the activation function, $MSE_f$ is for the second neural net,a single layer neural network, with Naiver-Stokes function(see below) as the activation function. The $MSE_f$ is included to the total loss, this applys the Naiver-Stokes function by balancing the $MSE_U$ to improve the final loss $MSE=MSE_u+MSE_f$ computed\n",
    "\n",
    "\n",
    "Where the ${t_u^i,x_u^i,u^i}$ denote the initial and boundary training data on $u(t,x)$ and $[t_f^i,x_f^i]_{i=1}^{N_f}$ specify the collocations points ${MSE}_u$ corresponds to the initial and boundary data while ${MSE}_f$ enforces the structure imposed by equation  at a finite set of collocation points.\n",
    "\n",
    "In 2-D Naiver-Stoke equation, we can define that\n",
    "$$u_t+λ_1(u*u_x +v*u_y)=-p_x+λ_2(u_{xx} +u_{yy})$$\n",
    "$$v_t+λ_1(u*v_x +v*v_y)=-p_y+λ_2(v_{xx} +v_{yy})$$\n",
    "\n",
    "hence we can derive $f(x,y,t)$ and $g(x,y,t)$, where g is simillar as f but it is the result for vertical flow v, \n",
    "\n",
    "$$f :=u_t +λ_1(u*u_x +v*u_y)+p_x −λ_2(u_{xx} +u_{yy})$$\n",
    "\n",
    "$$g:=v_t +λ_1(u*v_x +v*v_y)+p_y −λ_2(v_{xx} +v_{yy})$$\n",
    "\n",
    "On the equations $λ_1$ and $λ_2$ are the weighting parameter, unknown for now, solutions to the Navier–Stokes equations are searched in the set of divergence-free functions; i.e.,\n",
    "$$u_x +v_y =0$$\n",
    "\n",
    "This extra equation is the continuity equation for incompressible fluids that describes the conservation of mass of the fluid.\n",
    "We make the assumption that$u=ψ_y, v=−ψ_x$ for latent function $ψ(t,x,y)$. \n",
    "\n",
    "Under this assumption, the continuity equation $u_x +v_y =0$ will be automatically satisfied.\n",
    "\n",
    "Given the measurements {$t^i,x^i,y^i,u^i,v^i,p^i$} of the velocity field, what we need is the parameters λ as well as the pressure $p$, by taking the result the Naiver-Stoke parameter,[$f(x,y,t),g(x,y,t),ψ(x,y,t),p(x,y,t)$] and λ can be trained by minimizing the MSE loss\n",
    "\n",
    "$$MSE = \\frac{1}{N}\\sum_{i=1}^{N}(\\lvert{u(t^i,x^i,y^i)-u^i}\\rvert+\\lvert{v(t^i,x^i,y^i)-v^i}\\rvert )+ \\frac{1}{N}\\sum_{i=1}^{N}(\\lvert{f(t^i,x^i,y^i)^2}\\rvert+\\lvert{g(t^i,x^i,y^i)^2}\\rvert )$$\n",
    "\n",
    "The following are the the attributes in the code:\n",
    "\n",
    "$x$:horizontal flow of fluid\n",
    "\n",
    "$y$:vertical flow of fluid\n",
    "\n",
    "$t$:time stamp\n",
    "\n",
    "$u$:x component of the velocity field\n",
    "\n",
    "$v$:y component of the velocity field\n",
    "\n",
    "$p$:pressure\n",
    "\n",
    "$f$:horizontal result of the neural network\n",
    "\n",
    "$g$:vertical result of the neural network\n",
    "\n",
    "In the neural network below, x,y,and t are the input, v,p"
   ]
  },
  {
   "attachments": {
    "Neural_network_example.svg": {
     "image/svg+xml": [
      "<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="330"
   height="440"
   id="svg2368"
   sodipodi:version="0.32"
   inkscape:version="0.46dev+devel"
   sodipodi:docname="Neural_network_example.svg"
   inkscape:output_extension="org.inkscape.output.svg.inkscape"
   version="1.0">
  <defs
     id="defs2370">
    <marker
       style="overflow:visible"
       id="TriangleOutS"
       orient="auto"
       refY="0"
       refX="0">
      <path
         style="fill-rule:evenodd;stroke:#000000;stroke-width:1pt;marker-start:none"
         id="path3322"
         transform="scale(0.2,0.2)"
         d="M 5.77,0 L -2.88,5 L -2.88,-5 L 5.77,0 z" />
    </marker>
  </defs>
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="2.8"
     inkscape:cx="161.5826"
     inkscape:cy="358.39695"
     inkscape:document-units="px"
     inkscape:current-layer="layer1"
     showgrid="false"
     inkscape:window-width="1280"
     inkscape:window-height="1003"
     inkscape:window-x="0"
     inkscape:window-y="0"
     inkscape:showpageshadow="false" />
  <metadata
     id="metadata2373">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Livello 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-212.14285,-322.44839)">
    <g
       id="g2406"
       transform="translate(197.51413,362.12677)">
      <path
         style="opacity:1;fill:#ab10d2;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1"
         id="path3160"
         transform="translate(177.88048,90.628287)"
         d="M 143.96258,136.44534 C 143.96771,152.31767 131.10208,165.18745 115.22976,165.18745 C 99.357431,165.18745 86.491806,152.31767 86.496935,136.44534 C 86.491806,120.57301 99.357431,107.70323 115.22976,107.70323 C 131.10208,107.70323 143.96771,120.57301 143.96258,136.44534 z" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:8;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6080"
         d="M 190.27566,343.52799 L 275.58155,248.74367" />
      <path
         style="opacity:1;fill:#047391;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1"
         id="path3166"
         transform="translate(64.551296,218.27594)"
         d="M 143.96258,136.44534 C 143.96771,152.31767 131.10208,165.18745 115.22976,165.18745 C 99.357431,165.18745 86.491806,152.31767 86.496935,136.44534 C 86.491806,120.57301 99.357431,107.70323 115.22976,107.70323 C 131.10208,107.70323 143.96771,120.57301 143.96258,136.44534 z" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:10;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6076"
         d="M 180.79723,185.84134 L 266.10312,217.72334" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:10;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6078"
         d="M 69.641074,299.58253 L 154.08528,343.52799" />
      <path
         style="opacity:1;fill:#047391;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1"
         id="path3156"
         transform="translate(64.551296,48.485249)"
         d="M 143.96258,136.44534 C 143.96771,152.31767 131.10208,165.18745 115.22976,165.18745 C 99.357431,165.18745 86.491806,152.31767 86.496935,136.44534 C 86.491806,120.57301 99.357431,107.70323 115.22976,107.70323 C 131.10208,107.70323 143.96771,120.57301 143.96258,136.44534 z" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:5;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6086"
         d="M 70.502751,297.85918 L 158.39367,203.93653" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:5;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6088"
         d="M 187.69063,265.97718 L 268.68815,235.81853" />
      <path
         style="opacity:1;fill:#047391;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1"
         id="path3164"
         transform="translate(64.551296,133.3806)"
         d="M 143.96258,136.44534 C 143.96771,152.31767 131.10208,165.18745 115.22976,165.18745 C 99.357431,165.18745 86.491806,152.31767 86.496935,136.44534 C 86.491806,120.57301 99.357431,107.70323 115.22976,107.70323 C 131.10208,107.70323 143.96771,120.57301 143.96258,136.44534 z" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:8;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6084"
         d="M 71.364427,300.44421 L 150.63859,277.17896" />
      <path
         style="opacity:1;fill:#00a33d;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1"
         id="path3162"
         transform="translate(-49.082537,162.22065)"
         d="M 143.96258,136.44534 C 143.96771,152.31767 131.10208,165.18745 115.22976,165.18745 C 99.357431,165.18745 86.491806,152.31767 86.496935,136.44534 C 86.491806,120.57301 99.357431,107.70323 115.22976,107.70323 C 131.10208,107.70323 143.96771,120.57301 143.96258,136.44534 z" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:8;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6082"
         d="M 73.087778,163.43779 L 158.39367,247.02032" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:5;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6090"
         d="M 185.1056,103.98216 L 275.58155,204.79821" />
      <path
         style="opacity:1;fill:#047391;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1"
         id="path3158"
         transform="translate(64.551296,-36.410076)"
         d="M 143.96258,136.44534 C 143.96771,152.31767 131.10208,165.18745 115.22976,165.18745 C 99.357431,165.18745 86.491806,152.31767 86.496935,136.44534 C 86.491806,120.57301 99.357431,107.70323 115.22976,107.70323 C 131.10208,107.70323 143.96771,120.57301 143.96258,136.44534 z" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:10;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path3168"
         d="M 69.641074,154.82102 L 154.08528,109.15221" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:10;stroke-linecap:butt;stroke-linejoin:miter;marker-end:url(#TriangleOutS);stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         id="path6092"
         d="M 71.364424,157.40605 L 153.22361,178.94793" />
      <path
         style="opacity:1;fill:#00a33d;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1"
         id="path2384"
         transform="translate(-49.082537,19.949868)"
         d="M 143.96258,136.44534 C 143.96771,152.31767 131.10208,165.18745 115.22976,165.18745 C 99.357431,165.18745 86.491806,152.31767 86.496935,136.44534 C 86.491806,120.57301 99.357431,107.70323 115.22976,107.70323 C 131.10208,107.70323 143.96771,120.57301 143.96258,136.44534 z" />
    </g>
    <text
       xml:space="preserve"
       style="font-size:24px;font-style:normal;font-weight:normal;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:4;stroke-linecap:square;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;font-family:Helvetica LT Std"
       x="259.43243"
       y="392.73257"
       id="text2425"><tspan
         sodipodi:role="line"
         id="tspan2427"
         x="259.43243"
         y="392.73257"><tspan
           style="font-style:normal;font-family:Arial"
           id="tspan2439">input</tspan></tspan><tspan
         sodipodi:role="line"
         x="259.43243"
         y="422.73257"
         id="tspan2453"><tspan
           style="font-style:normal;font-family:Arial"
           id="tspan2455">layer</tspan></tspan></text>
    <text
       id="text2441"
       y="392.73257"
       x="375.34995"
       style="font-size:24px;font-style:normal;font-weight:normal;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:4;stroke-linecap:square;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;font-family:Helvetica LT Std"
       xml:space="preserve"><tspan
         y="392.73257"
         x="375.34995"
         id="tspan2443"
         sodipodi:role="line"><tspan
           id="tspan2445"
           style="font-style:normal;font-family:Arial">hidden</tspan></tspan><tspan
         y="422.73257"
         x="375.34995"
         sodipodi:role="line"
         id="tspan2457"><tspan
           style="font-style:normal;font-family:Arial"
           id="tspan2459">layer</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:24px;font-style:normal;font-weight:normal;text-align:center;text-anchor:middle;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:4;stroke-linecap:square;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;font-family:Helvetica LT Std"
       x="489.9491"
       y="392.73257"
       id="text2447"><tspan
         sodipodi:role="line"
         id="tspan2449"
         x="489.9491"
         y="392.73257"><tspan
           style="font-style:normal;font-family:Arial"
           id="tspan2451">output</tspan></tspan><tspan
         sodipodi:role="line"
         x="489.9491"
         y="422.73257"
         id="tspan2461"><tspan
           style="font-style:normal;font-family:Arial"
           id="tspan2463">layer</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:27.67938614px;font-style:normal;font-weight:normal;text-align:center;text-anchor:middle;opacity:1;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:4;stroke-linecap:square;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;font-family:Helvetica LT Std"
       x="377.21719"
       y="351.12262"
       id="text2465"><tspan
         id="tspan2471"
         sodipodi:role="line"
         x="377.21719"
         y="351.12262"
         style="font-style:normal;font-family:Arial">A simple neural network</tspan></text>
  </g>
</svg>
"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\n",
    "\n",
    "An neural network is based on a collection of artificial neurons, which creates the model simillarly to a brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\n",
    "The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges.\n",
    "\n",
    "Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection, hence improves the final loss function. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times, the size of the input layer and the output layer has to be the same size with the number of inputs and outputs.\n",
    "\n",
    "Simplified view of a feedforward artificial neural network we'll be using later on\n",
    "![Neural_network_example.svg](attachment:Neural_network_example.svg)\n",
    "\n",
    "In the first neural net\n",
    "\n",
    "**``xavier_init``** function is used to initialize the weight matrices with [``tf.random.truncated_normal``](https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal) function generating random variable matrices\n",
    "\n",
    "**``initialize_NN``** function is implemented to map the weights and biases into the shape and size of the layers and neurons applyed later on, but in this case, biases are all set to zeros, the weights are from the **``xavier_init``** function above.For instance if the layer has been set to [3,20,20,2], the output of the ``initialize_NN`` function will be weight matrices with in the shape of [3,20],[20,20] and [20,2].\n",
    "\n",
    "In **``neural_net``** function a lambda function is applied to the neural net $H=2.0*(X - lb)/(ub - lb) - 1.0$, where $X$ is the wrap of all input variables, $lb$,$ub$ are the minimum and maximum variable in $X$, the activation function applied is hyperbolic tangent or $tanh$, inside the function is the lambda is updated for each layer $H=tanh(H*[weights]+[biases])$ where the weights and biases belong to the corresponding layer, H has the shape of ``[training size,neurons]``\n",
    "\n",
    "At the output layer, the output function is $Y=H*[weights]+[biases]$, uses the previous lambda value $H$. The output $Y$ is the wrap up of $\\psi$ and $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates the model class, and initialize the layers, parameters and placeholders, the **``xavier_init``** and **``initialize_NN``** are to construct the full structure of the weight and bias matrices, making them the same length and width with the layers setting. \n",
    "\n",
    "**``neural_net``** is a simple feedforward nueral network, with the $x$,$y$ and $t$ as the inputs, $ψ$ and $p$ are the outputs\n",
    "\n",
    "**``net_NS``** is a second neural net, to proceed the Naiver-Stoke equation,it take $x$,$y$ and $t$ as the inputs, mutiplying devriatives based on equation [1] and [2]. The outputs are the $u$,$v$,$p$ prediction for further testing and verifaction\n",
    "\n",
    "The **``training``** function in the original code, has applied Adam optimizer and scipy optimizer(with L-BFGS method), but the latter is no longer available for tensorflow 2, the simillar replacement in tfp is not very useful too, hence only the Adam optimizer is implemented for now. Adam optimizer would optimize the function, updating the lambda in [1] and [2], as well as the final loss value\n",
    "\n",
    "Finally, the **``prediction``** function is for testing, it will apply the latest training function with the testing data, the predicted results are $u$, $v$ and $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../Utilities/')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import latex\n",
    "import csv\n",
    "import contextlib\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "import meshio\n",
    "from itertools import product, combinations\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow_probability as tfp\n",
    "import external_optimizer as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN:       \n",
    "    # Initialize the class \n",
    "    def __init__(self, x, y, t, u, v, layers):\n",
    "        \n",
    "        X = np.concatenate([x, y, t], 1)\n",
    "        \n",
    "        self.lb = X.min(0)\n",
    "        self.ub = X.max(0)\n",
    "                \n",
    "        self.X = X\n",
    "        \n",
    "        self.x = X[:,0:1]\n",
    "        self.y = X[:,1:2]\n",
    "        self.t = X[:,2:3]\n",
    "        \n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NN\n",
    "        self.weights, self.biases = self.initialize_NN(layers)        \n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
    "        self.lambda_2 = tf.Variable([0.0], dtype=tf.float32)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        self.x_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.x.shape[1]])\n",
    "        self.y_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.y.shape[1]])\n",
    "        self.t_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.t.shape[1]])\n",
    "        \n",
    "        self.u_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.u.shape[1]])\n",
    "        self.v_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.v.shape[1]])\n",
    "        #f_u_pred and f_v_pred corresponds to the f and g in the front\n",
    "        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred = self.net_NS(self.x_tf, self.y_tf, self.t_tf)\n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                    tf.reduce_sum(tf.square(self.v_tf - self.v_pred)) + \\\n",
    "                    tf.reduce_sum(tf.square(self.f_u_pred)) + \\\n",
    "                    tf.reduce_sum(tf.square(self.f_v_pred))                 \n",
    "        self.lossu=tf.reduce_sum(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                   tf.reduce_sum(tf.square(self.v_tf - self.v_pred))\n",
    "        self.lossf=tf.reduce_sum(tf.square(self.f_u_pred)) + \\\n",
    "                    tf.reduce_sum(tf.square(self.f_v_pred)) \n",
    "        \n",
    "        self.optimizer = opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                     method = 'L-BFGS-B', \n",
    "                                                     options = {'maxiter': 50000,\n",
    "                                                                'maxfun': 50000,\n",
    "                                                                'maxcor': 50,\n",
    "                                                                'maxls': 50,\n",
    "                                                                'ftol' : 1.0 * np.finfo(float).eps})        \n",
    "        \n",
    "        self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)                    \n",
    "        \n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        #Initialize the content of the weight matrix\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert   \n",
    "    def net_NS(self, x, y, t):\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        psi_and_p = self.neural_net(tf.concat([x,y,t], 1), self.weights, self.biases)\n",
    "        psi = psi_and_p[:,0:1]\n",
    "        p = psi_and_p[:,1:2]\n",
    "        u = tf.gradients(psi, y)[0]\n",
    "        v = -tf.gradients(psi, x)[0]\n",
    "        \n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_y = tf.gradients(u, y)[0]\n",
    "        \n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        u_yy = tf.gradients(u_y, y)[0]\n",
    "        \n",
    "        v_t = tf.gradients(v, t)[0]\n",
    "        v_x = tf.gradients(v, x)[0]\n",
    "        v_y = tf.gradients(v, y)[0]\n",
    "        \n",
    "        v_xx = tf.gradients(v_x, x)[0]\n",
    "        v_yy = tf.gradients(v_y, y)[0]\n",
    "        \n",
    "        p_x = tf.gradients(p, x)[0]\n",
    "        p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "        f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_2*(u_xx + u_yy) \n",
    "        f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_2*(v_xx + v_yy) \n",
    "        \n",
    "        return u, v, p, f_u, f_v\n",
    "    \n",
    "    def train(self, nIter): \n",
    "        finl=[]\n",
    "        lu=[]\n",
    "        lf=[]\n",
    "        tf_dict = {self.x_tf: self.x, self.y_tf: self.y, self.t_tf: self.t,\n",
    "                   self.u_tf: self.u, self.v_tf: self.v}        \n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_value_u = self.sess.run(self.lossu,tf_dict)\n",
    "                loss_value_f = self.sess.run(self.lossf,tf_dict)\n",
    "                lambda_1_value = self.sess.run(self.lambda_1)\n",
    "                lambda_2_value = self.sess.run(self.lambda_2)\n",
    "                finl.append(loss_value)\n",
    "                lu.append(loss_value_u)\n",
    "                lf.append(loss_value_f)\n",
    "                print('It: %d, Loss: %.3e, Loss_u:%.3e , Loss_f:%.3e , l1: %.5f, l2: %.5f, Time: %.2f' % \n",
    "                      (it, loss_value,loss_value_u,loss_value_f ,lambda_1_value, lambda_2_value, elapsed))\n",
    "                start_time = time.time()\n",
    "        plt.figure(1) \n",
    "        plt.plot(np.arange(0,nIter/10),finl)\n",
    "        plt.figure(2)\n",
    "        plt.plot(np.arange(0,nIter/10),lu)\n",
    "        plt.figure(3)\n",
    "        plt.plot(np.arange(0,nIter/10),lf) \n",
    "        with open('names.csv', 'w') as csvfile:\n",
    "            fieldnames = ['finl','lu','lf']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for i in range(round(nIter/10)):\n",
    "                writer.writerow({\"finl\":finl[i],'lu':lu[i],'lf':lf[i]})\n",
    "    def callback(self, loss, lambda_1, lambda_2):\n",
    "        print('Loss: %.3e, l1: %.3f, l2: %.5f' % (loss, lambda_1, lambda_2))  \n",
    "        \n",
    "    def predict(self, x_star, y_star, t_star):\n",
    "        \n",
    "        tf_dict = {self.x_tf: x_star, self.y_tf: y_star, self.t_tf: t_star}\n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
    "        \n",
    "        return u_star, v_star, p_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the training datasize N_train and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    N_train = 3000\n",
    "    \n",
    "    layers = [3, 40, 40, 40, 40, 40, 40, 40, 40, 40, 2] #11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Stage for noiseless data\n",
    "Import the data from file and process it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # Load Data\n",
    "    # ——————————————————————————————————————————————————————————————————\n",
    "    mesh = meshio.read(\n",
    "    filename='/Users/howardxu/work/neuro-net/Physics-informed-neural-networks/main/Data/rec000068.vtu', \n",
    "    # string, os.PathLike, or a buffer/open file\n",
    "      # optional if filename is a path; inferred from extension\n",
    "    )    \n",
    "    \n",
    "    x=mesh.points[:,0]\n",
    "    y=mesh.points[:,1]\n",
    "    t=np.arange(0,68*1.467451833203625E-004,1.467451833203625E-004);\n",
    "\n",
    "    u=mesh.point_data['flds1']#x\n",
    "    v=mesh.point_data['flds2']#y\n",
    "    p=mesh.point_data['flds3']#pressure\n",
    "    N = x.shape[0]\n",
    "    T = t.shape[0]\n",
    "   # ——————————————————————————————————————————————————————————————————    \n",
    "\n",
    "    x=x.flatten()[:,None]\n",
    "    y=y.flatten()[:,None]\n",
    "    t=t.flatten()[:,None]\n",
    "    XX = np.tile(x, (1,T)) # N x T\n",
    "    YY = np.tile(y, (1,T)) # N x T\n",
    "    TT = np.tile(t, (N,1)) # N x T\n",
    "    UU = np.tile(u, (1,T))\n",
    "    VV = np.tile(v, (1,T))\n",
    "    PP = np.tile(p, (1,T))\n",
    "    \n",
    "    x = XX.flatten()[:,None] # NT x 1\n",
    "    y = YY.flatten()[:,None] # NT x 1\n",
    "    t = TT.flatten()[:,None] # NT x 1\n",
    "    \n",
    "    u = UU.flatten()[:,None] # NT x 1\n",
    "    v = VV.flatten()[:,None] # NT x 1\n",
    "    p = PP.flatten()[:,None] # NT \n",
    "    \n",
    "    ######################################################################\n",
    "    ######################## Noiseless Data ###############################\n",
    "    ##x####################################################################\n",
    "    # Training Data /randomly taken within the range of all timestamp  \n",
    "    idx = np.random.choice(N*T, N_train, replace=False)\n",
    "    x_train = x[idx,:]\n",
    "    y_train = y[idx,:]\n",
    "    t_train = t[idx,:]\n",
    "    u_train = u[idx,:]\n",
    "    v_train = v[idx,:]\n",
    "\n",
    "\n",
    "    # Apply the Training\n",
    "    model = PhysicsInformedNN(x_train, y_train, t_train, u_train, v_train, layers)\n",
    "    #The train iterations\n",
    "    epoch=30000\n",
    "    model.train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stage for noiseless data\n",
    "Choose a certain time stamp's data for testing,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    t=np.arange(0,68*1.467451833203625E-004,1.467451833203625E-004);\n",
    "    TT = np.tile(t, (N,1))\n",
    "    # Test Data /specificly taken at a certain time stamp, in this case 10\n",
    "    snap = np.array([1])\n",
    "    x_star = XX[:,snap]\n",
    "    y_star = YY[:,snap]\n",
    "    t_star = TT[:,snap]\n",
    "    \n",
    "    u_star = UU[:,snap]\n",
    "    v_star = VV[:,snap]\n",
    "    p_star = PP[:,snap]\n",
    "    \n",
    "    # Prediction\n",
    "    u_pred, v_pred, p_pred = model.predict(x_star, y_star, t_star)\n",
    "    lambda_1_value = model.sess.run(model.lambda_1)\n",
    "    lambda_2_value = model.sess.run(model.lambda_2)\n",
    "    \n",
    "    # Error\n",
    "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
    "    error_p = np.linalg.norm(p_star-p_pred,2)/np.linalg.norm(p_star,2)\n",
    "\n",
    "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
    "    error_lambda_2 = np.abs(lambda_2_value - 0.01)/0.01 * 100\n",
    "    print('Error u: %e' % (error_u))    \n",
    "    print('Error v: %e' % (error_v))    \n",
    "    print('Error p: %e' % (error_p))    \n",
    "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "    print('Error l2: %.5f%%' % (error_lambda_2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the plotting function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_solution(X_star, u_star, index):\n",
    "    \n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)\n",
    "    nn = 200\n",
    "    x = np.linspace(lb[0], ub[0], nn)\n",
    "    y = np.linspace(lb[1], ub[1], nn)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    \n",
    "    U_star = griddata(X_star, u_star.flatten(), (X, Y), method='cubic')\n",
    "    \n",
    "    plt.figure(index)\n",
    "    plt.pcolor(X,Y,U_star, cmap = 'jet')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    \n",
    "def axisEqual3D(ax):\n",
    "    extents = np.array([getattr(ax, 'get_{}lim'.format(dim))() for dim in 'xyz'])\n",
    "    sz = extents[:,1] - extents[:,0]\n",
    "    centers = np.mean(extents, axis=1)\n",
    "    maxsize = max(abs(sz))\n",
    "    r = maxsize/4\n",
    "    for ctr, dim in zip(centers, 'xyz'):\n",
    "        getattr(ax, 'set_{}lim'.format(dim))(ctr - r, ctr + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction plot of original test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x1=mesh.points[:,0]\n",
    "    y1=mesh.points[:,1]\n",
    "    X_star=np.concatenate([x1.flatten()[:,None],y1.flatten()[:,None]],axis=1)\n",
    "    plot_solution(X_star, u_star, 1)\n",
    "    plot_solution(X_star, v_star, 2)\n",
    "    plot_solution(X_star, p_star, 3)\n",
    "    plot_solution(X_star, p_star, 4)\n",
    "    plot_solution(X_star, p_star - p_pred, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction plot of noiseless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    x1=mesh.points[:,0]\n",
    "    y1=mesh.points[:,1]\n",
    "    # Prediction plot\n",
    "    X_star=np.concatenate([x1.flatten()[:,None],y1.flatten()[:,None]],axis=1)\n",
    "    plot_solution(X_star, u_pred, 1)\n",
    "    plot_solution(X_star, v_pred, 2)\n",
    "    plot_solution(X_star, p_pred, 3)\n",
    "    plot_solution(X_star, p_star, 4)\n",
    "    plot_solution(X_star, p_star - p_pred, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(np.linalg.norm(p_star-p_pred,2)/np.linalg.norm(p_star,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training stage for noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    noise = 0.01        \n",
    "    u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "    v_train = v_train + noise*np.std(v_train)*np.random.randn(v_train.shape[0], v_train.shape[1])    \n",
    "    # Training\n",
    "    model = PhysicsInformedNN(x_train, y_train, t_train, u_train, v_train, layers)\n",
    "    model.train(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing stage for noisy data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    lambda_1_value_noisy = model.sess.run(model.lambda_1)\n",
    "    lambda_2_value_noisy = model.sess.run(model.lambda_2)\n",
    "    u_pred_noisy, v_pred_noisy, p_pred_noisy = model.predict(x_star, y_star, t_star)\n",
    "    \n",
    "    \n",
    "    error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\n",
    "    error_lambda_2_noisy = np.abs(lambda_2_value_noisy - 0.01)/0.01 * 100\n",
    "    error_u_noisy = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    error_v_noisy = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
    "    error_p_noisy = np.linalg.norm(p_star-p_pred,2)/np.linalg.norm(p_star,2)\n",
    "\n",
    "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
    "    error_lambda_2 = np.abs(lambda_2_value - 0.01)/0.01 * 100\n",
    "    print('Error u_noisy: %e' % (error_u_noisy))    \n",
    "    print('Error v_noisy: %e' % (error_v_noisy))    \n",
    "    print('Error p_noisy: %e' % (error_p_noisy))       \n",
    "    print('Error l1_noisy: %.5f%%' % (error_lambda_1_noisy))                             \n",
    "    print('Error l2_noisy: %.5f%%' % (error_lambda_2_noisy))     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction plot of noiseless prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    plot_solution(X_star, u_pred_noisy, 1)\n",
    "    plot_solution(X_star, v_pred_noisy, 2)\n",
    "    plot_solution(X_star, p_pred_noisy, 3)\n",
    "    plot_solution(X_star, p_star, 4)\n",
    "    plot_solution(X_star, p_star - p_pred_noisy, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
