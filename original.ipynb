{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "original.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/56aaaaa/Physics-informed-neural-networks/blob/master/original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpXyiNeflHTX",
        "colab_type": "text"
      },
      "source": [
        "# Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations\n",
        "\n",
        "## Author:M. Raissi, P. Perdikaris, G.E. Karniadakis\n",
        "\n",
        "[github](https://github.com/maziarraissi/PINNs)\n",
        "\n",
        "This piece of code is to predict the velocity fields and the pressure of a fluid system's flow status, based on Naiver-Stoke 2D equation,the results are demonstrated in **tensorflow 2 using the original data**. As the original code was written in tensorflow 1, variations were made to compat the latest version\n",
        "\n",
        "In this work, we consider parametrized and nonlinear partial differential equations of the general form as\n",
        "\n",
        "$u_t + N[u; \\lambda] = 0, x \\in {\\Omega}􏰀, t \\in [0, T]$\n",
        "\n",
        "where $u_t$ denotes the latent solution, $N[u; λ]$ is a nonlinear operator parameterized by λ, a weight variable and $\\Omega$ is a subset of $\\mathbb{R}^D$\n",
        "\n",
        "The current code is for the continuous time model, hence from the above we can define\n",
        "$f :=u_t +N[u]$\n",
        "by proceeding $u(t,x)$ to the neural network, we can get $f(t,x)$ as the result.\n",
        "\n",
        "This network can be derived by applying the chain rule for differentiating compositions of functions using automatic differentiation, and has the same parameters as the network representing $u(t,x)$, albeit with different activation functions due to the action of the differential operator $N$. But both $u(t,x)$ and $f(t,x)$ can be operated by minimizing the mean squared error loss where:\n",
        "$$MSE=MSE_u+MSE_f$$\n",
        "$$MSE_U = \\frac{1}{N_u}\\sum_{i=1}^{N_u}\\lvert u(t_u^i,x_u^i)-u^i \\rvert^2$$\n",
        "$$MSE_f = \\frac{1}{N_f}\\sum_{i=1}^{N_f}\\lvert f(t_f^i,x_f^i) \\rvert^2$$\n",
        "\n",
        "As the $MSE_U$ represent the loss for the first neural net,a multi-layered neural network, with hyperbolic tangent as the activation function, $MSE_f$ is for the second neural net,a single layer neural network, with Naiver-Stokes function(see below) as the activation function. The $MSE_f$ is included to the total loss, this applys the Naiver-Stokes function by balancing the $MSE_u$ to improve the final loss $MSE=MSE_u+MSE_f$ computed\n",
        "\n",
        "\n",
        "Where the ${t_u^i,x_u^i,u^i}$ denote the initial and boundary training data on $u(t,x)$ and $[t_f^i,x_f^i]_{i=1}^{N_f}$ specify the collocations points ${MSE}_u$ corresponds to the initial and boundary data while ${MSE}_f$ enforces the structure imposed by equation at a finite set of collocation points.\n",
        "\n",
        "In 2-D Naiver-Stoke equation, we can define that\n",
        "$$u_t+λ_1(u*u_x +v*u_y)=-p_x+λ_2(u_{xx} +u_{yy})$$\n",
        "$$v_t+λ_1(u*v_x +v*v_y)=-p_y+λ_2(v_{xx} +v_{yy})$$\n",
        "\n",
        "hence we can derive $f(x,y,t)$ and $g(x,y,t)$, where g is simillar as f but it is the result for vertical flow v, \n",
        "\n",
        "$$f :=u_t +λ_1(u*u_x +v*u_y)+p_x −λ_2(u_{xx} +u_{yy})$$\n",
        "\n",
        "$$g:=v_t +λ_1(u*v_x +v*v_y)+p_y −λ_2(v_{xx} +v_{yy})$$\n",
        "\n",
        "On the equations $λ_1$ and $λ_2$ are the weighting parameter, unknown for now, solutions to the Navier–Stokes equations are searched in the set of divergence-free functions; i.e.,\n",
        "$$u_x +v_y =0$$\n",
        "\n",
        "This extra equation is the continuity equation for incompressible fluids that describes the conservation of mass of the fluid.\n",
        "We make the assumption that$u=ψ_y, v=−ψ_x$ for latent function $ψ(t,x,y)$. \n",
        "\n",
        "Under this assumption, the continuity equation $u_x +v_y =0$ will be automatically satisfied.\n",
        "\n",
        "Given the measurements {$t^i,x^i,y^i,u^i,v^i,p^i$} of the velocity field, what we need is the parameters λ as well as the pressure $p$, by taking the result the Naiver-Stoke parameter,[$f(x,y,t),g(x,y,t),ψ(x,y,t),p(x,y,t)$] and λ can be trained by minimizing the MSE loss\n",
        "\n",
        "$$MSE = \\frac{1}{N}\\sum_{i=1}^{N}(\\lvert{u(t^i,x^i,y^i)-u^i}\\rvert+\\lvert{v(t^i,x^i,y^i)-v^i}\\rvert )+ \\frac{1}{N}\\sum_{i=1}^{N}(\\lvert{f(t^i,x^i,y^i)^2}\\rvert+\\lvert{g(t^i,x^i,y^i)^2}\\rvert )$$\n",
        "\n",
        "The following are the the attributes in the code:\n",
        "\n",
        "$x$:horizontal flow of fluid\n",
        "\n",
        "$y$:vertical flow of fluid\n",
        "\n",
        "$t$:time stamp\n",
        "\n",
        "$u$:x component of the velocity field\n",
        "\n",
        "$v$:y component of the velocity field\n",
        "\n",
        "$p$:pressure\n",
        "\n",
        "$f$:horizontal result of the neural network\n",
        "\n",
        "$g$:vertical result of the neural network\n",
        "\n",
        "In the neural network below, x,y,and t are the input, v,p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qV4ifOslHTZ",
        "colab_type": "text"
      },
      "source": [
        "### Neural Networks\n",
        "\n",
        "Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\n",
        "\n",
        "An neural network is based on a collection of artificial neurons, which creates the model simillarly to a brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it.\n",
        "The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges.\n",
        "\n",
        "Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection, hence improves the final loss function. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times, the size of the input layer and the output layer has to be the same size with the number of inputs and outputs.\n",
        "\n",
        "Simplified view of a feedforward artificial neural network we'll be using later on\n",
        "![Neural_network_example.svg](attachment:Neural_network_example.svg)\n",
        "\n",
        "In the first neural net is a multi-layer feedforward neural network, producing prediction of pressure and $\\psi$\n",
        "\n",
        "**``xavier_init``** function is used to initialize the weight matrices with [``tf.random.truncated_normal``](https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal) function generating random variable matrices\n",
        "\n",
        "**``initialize_NN``** function is to map the weights and biases matrices into the shape and size of the layers and neurons applyed, in the function, biases are all set to zeros so it won't be a concern, the weights are from the **``xavier_init``** function above. For instance if the layer has been set to [3,20,20,2], the output of the **``initialize_NN``** function will be weight matrices in the shape of [3,20],[20,20] and [20,2].\n",
        "\n",
        "In **``neural_net``** function a lambda function is applied to the neural net, initialized by $H=2.0*(X - lb)/(ub - lb) - 1.0$, where $X$ is the wrap up of all input variables($x$,$y$ and $t$), $lb$,$ub$ are the minimum and maximum variable in $X$, the activation function applied is hyperbolic tangent (or $tanh$), inside the function is the lambda is updated for each layer $H=tanh(H*[weights]+[biases])$ where the weights and biases belong to the corresponding layer, H has the shape of ``[training size,neurons]``\n",
        "\n",
        "At the output layer, the output function is $Y=H*[weights]+[biases]$, uses the previous lambda value $H$. The output $Y$ is the wrap up of $\\psi$ and $p$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_94L6yOlHTa",
        "colab_type": "text"
      },
      "source": [
        "For the second neural net, it comprehend of a single layer feedforward neural network, with the $MSE_u$ and $MSE_f$ as the output\n",
        "\n",
        "**``neural_net``** is a simple feedforward nueral network, with the $x$,$y$ and $t$ as the inputs, $ψ$ and $p$ are the outputs\n",
        "\n",
        "**``net_NS``** is a second neural net, to proceed the Naiver-Stoke equation,it take $x$,$y$ and $t$ as the inputs, mutiplying devriatives based on equation for $MSE_f$. The outputs are the $u$,$v$,$p$ prediction for further testing and verifaction\n",
        "\n",
        "The **``training``** function in the original code, has applied Adam optimizer and scipy optimizer(with L-BFGS method), but the latter is no longer available for tensorflow 2, the simillar replacement in tensorflow probability is not very useful too, hence only the Adam optimizer is implemented for now. Adam optimizer would optimize the function, updating the lambda in $f$ and $g$, as well as the final $MSE_f$ loss value. Where the absent of the L-BFGS optimize will lead to a rise of the lambda error, in the original code (excuted in tensorflow 1 with the data from the paper), applying only the Adam optimizer will have only minor affect on the final prediction\n",
        "\n",
        "Finally, the **``prediction``** function is for testing, it will apply the latest training function with the testing data(), the predicted results are $u$, $v$ and $p$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQYxPZolHTb",
        "colab_type": "text"
      },
      "source": [
        "First import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "dO35b7zKlHTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../../Utilities/')\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import latex\n",
        "import contextlib\n",
        "from scipy.interpolate import griddata\n",
        "import time\n",
        "import meshio\n",
        "from itertools import product, combinations\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "from plotting import newfig, savefig\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9NDvWxWlHTq",
        "colab_type": "text"
      },
      "source": [
        "Set seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "mBvebgiflHTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "tf.compat.v1.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSlIpxEhlHTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PhysicsInformedNN:       \n",
        "    # Initialize the class \n",
        "    def __init__(self, x, y, t, u, v, layers):\n",
        "        \n",
        "        X = np.concatenate([x, y, t], 1)\n",
        "        \n",
        "        self.lb = X.min(0)\n",
        "        self.ub = X.max(0)\n",
        "                \n",
        "        self.X = X\n",
        "        \n",
        "        self.x = X[:,0:1]\n",
        "        self.y = X[:,1:2]\n",
        "        self.t = X[:,2:3]\n",
        "        \n",
        "        self.u = u\n",
        "        self.v = v\n",
        "        \n",
        "        self.layers = layers\n",
        "        \n",
        "        # Initialize NN\n",
        "        self.weights, self.biases = self.initialize_NN(layers)        \n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
        "        self.lambda_2 = tf.Variable([0.0], dtype=tf.float32)\n",
        "        \n",
        "        # tf placeholders and graph\n",
        "        self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        self.x_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.x.shape[1]])\n",
        "        self.y_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.y.shape[1]])\n",
        "        self.t_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.t.shape[1]])\n",
        "        \n",
        "        self.u_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.u.shape[1]])\n",
        "        self.v_tf = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, self.v.shape[1]])\n",
        "        #f_u_pred and f_v_pred corresponds to the f and g in the front\n",
        "        self.u_pred, self.v_pred, self.p_pred, self.f_u_pred, self.f_v_pred = self.net_NS(self.x_tf, self.y_tf, self.t_tf)\n",
        "        \n",
        "        self.loss = tf.reduce_sum(tf.square(self.u_tf - self.u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.v_tf - self.v_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.f_u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.f_v_pred))                 \n",
        "        self.lossu=tf.reduce_sum(tf.square(self.u_tf - self.u_pred)) + \\\n",
        "                   tf.reduce_sum(tf.square(self.v_tf - self.v_pred))\n",
        "        self.lossf=tf.reduce_sum(tf.square(self.f_u_pred)) + \\\n",
        "                    tf.reduce_sum(tf.square(self.f_v_pred))        \n",
        "        \n",
        "        self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)                    \n",
        "        \n",
        "        init = tf.compat.v1.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "    def initialize_NN(self, layers):        \n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers) \n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)        \n",
        "        return weights, biases\n",
        "        \n",
        "    def xavier_init(self, size):\n",
        "        #Initialize the content of the weight matrix\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]        \n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "    \n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "        \n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "    \n",
        "    @tf.autograph.experimental.do_not_convert   \n",
        "    def net_NS(self, x, y, t):\n",
        "        lambda_1 = self.lambda_1\n",
        "        lambda_2 = self.lambda_2\n",
        "        psi_and_p = self.neural_net(tf.concat([x,y,t], 1), self.weights, self.biases)\n",
        "        psi = psi_and_p[:,0:1]\n",
        "        p = psi_and_p[:,1:2]\n",
        "        u = tf.gradients(psi, y)[0]\n",
        "        v = -tf.gradients(psi, x)[0]\n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_y = tf.gradients(u, y)[0]\n",
        "        \n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        u_yy = tf.gradients(u_y, y)[0]\n",
        "        \n",
        "        v_t = tf.gradients(v, t)[0]\n",
        "        v_x = tf.gradients(v, x)[0]\n",
        "        v_y = tf.gradients(v, y)[0]\n",
        "        \n",
        "        v_xx = tf.gradients(v_x, x)[0]\n",
        "        v_yy = tf.gradients(v_y, y)[0]\n",
        "        \n",
        "        p_x = tf.gradients(p, x)[0]\n",
        "        p_y = tf.gradients(p, y)[0]\n",
        "\n",
        "        f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_2*(u_xx + u_yy) \n",
        "        f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_2*(v_xx + v_yy) \n",
        "        \n",
        "        return u, v, p, f_u, f_v\n",
        "    \n",
        "    def train(self, nIter): \n",
        "        finl=[]\n",
        "        lu=[]\n",
        "        lf=[]\n",
        "        tf_dict = {self.x_tf: self.x, self.y_tf: self.y, self.t_tf: self.t,\n",
        "                   self.u_tf: self.u, self.v_tf: self.v}        \n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.sess.run(self.train_op_Adam, tf_dict)\n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                \n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                loss_value_u = self.sess.run(self.lossu,tf_dict)\n",
        "                loss_value_f = self.sess.run(self.lossf,tf_dict)\n",
        "                lambda_1_value = self.sess.run(self.lambda_1)\n",
        "                lambda_2_value = self.sess.run(self.lambda_2)\n",
        "                finl.append(loss_value)\n",
        "                lu.append(loss_value_u)\n",
        "                lf.append(loss_value_f)\n",
        "                print('It: %d, Loss: %.3e, Loss_u:%.3e , Loss_f:%.3e , l1: %.5f, l2: %.5f, Time: %.2f' % \n",
        "                      (it, loss_value,loss_value_u,loss_value_f ,lambda_1_value, lambda_2_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        plt.figure(1) \n",
        "        plt.plot(np.arange(0,nIter/10),finl)\n",
        "        plt.figure(2)\n",
        "        plt.plot(np.arange(0,nIter/10),lu)\n",
        "        plt.figure(3)\n",
        "        plt.plot(np.arange(0,nIter/10),lf) \n",
        "        with open('names.csv', 'w') as csvfile:\n",
        "            fieldnames = ['finl','lu','lf']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for i in range(round(nIter/10)):\n",
        "                writer.writerow({\"finl\":finl[i],'lu':lu[i],'lf':lf[i]})\n",
        "    def callback(self, loss, lambda_1, lambda_2):\n",
        "        print('Loss: %.3e, l1: %.3f, l2: %.5f' % (loss, lambda_1, lambda_2))  \n",
        "        \n",
        "    def predict(self, x_star, y_star, t_star):\n",
        "        \n",
        "        tf_dict = {self.x_tf: x_star, self.y_tf: y_star, self.t_tf: t_star}\n",
        "        \n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
        "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
        "        \n",
        "        return u_star, v_star, p_star"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLhI-IaClHT2",
        "colab_type": "text"
      },
      "source": [
        "Set up the training datasize N_train and layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "rIP4OASNlHT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\": \n",
        "    N_train = 1000\n",
        "    \n",
        "    layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 2] #11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCD8-CY6lHT-",
        "colab_type": "text"
      },
      "source": [
        "### Training Stage for noiseless data\n",
        "Import the data from file and process it for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EpLqvRNflHT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Load Data\n",
        "    # ——————————————————————————————————————————————————————————————————\n",
        "    data = scipy.io.loadmat('../Data/cylinder_nektar_wake.mat')\n",
        "           \n",
        "    U_star = data['U_star'] # N x 2 x T\n",
        "    P_star = data['p_star'] # N x T\n",
        "    t_star = data['t'] # T x 1\n",
        "    X_star = data['X_star'] # N x 2\n",
        "    \n",
        "    N = X_star.shape[0]\n",
        "    T = t_star.shape[0]\n",
        "    \n",
        "    # Rearrange Data \n",
        "    XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "    YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "    TT = np.tile(t_star, (1,N)).T # N x T\n",
        "    \n",
        "    UU = U_star[:,0,:] # N x T\n",
        "    VV = U_star[:,1,:] # N x T\n",
        "    PP = P_star # N x T\n",
        "    \n",
        "    x = XX.flatten()[:,None] # NT x 1\n",
        "    y = YY.flatten()[:,None] # NT x 1\n",
        "    t = TT.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    u = UU.flatten()[:,None] # NT x 1\n",
        "    v = VV.flatten()[:,None] # NT x 1\n",
        "    p = PP.flatten()[:,None] # NT x 1\n",
        "    \n",
        "    ######################################################################\n",
        "    ######################## Noiseless Data ###############################\n",
        "    ##x####################################################################\n",
        "    # Training Data /randomly taken within the range of all timestamp  \n",
        "    idx = np.random.choice(N*T, N_train, replace=False)\n",
        "    x_train = x[idx,:]\n",
        "    y_train = y[idx,:]\n",
        "    t_train = t[idx,:]\n",
        "    u_train = u[idx,:]\n",
        "    v_train = v[idx,:]\n",
        "\n",
        "\n",
        "    # Apply the Training\n",
        "    model = PhysicsInformedNN(x_train, y_train, t_train, u_train, v_train, layers)\n",
        "    #The train iterations\n",
        "    epoch=30000\n",
        "    model.train(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW9kw84XlHUD",
        "colab_type": "text"
      },
      "source": [
        "### Testing Stage for noiseless data\n",
        "Choose a certain time stamp's data for testing,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "q0mAdr9VlHUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    t=np.arange(0,68*1.467451833203625E-004,1.467451833203625E-004);\n",
        "    TT = np.tile(t, (N,1))\n",
        "    # Test Data /specificly taken at a certain time stamp, in this case 10\n",
        "    snap = np.array([1])\n",
        "    x_star = XX[:,snap]\n",
        "    y_star = YY[:,snap]\n",
        "    t_star = TT[:,snap]\n",
        "    \n",
        "    u_star = UU[:,snap]\n",
        "    v_star = VV[:,snap]\n",
        "    p_star = PP[:,snap]\n",
        "    \n",
        "    # Prediction\n",
        "    u_pred, v_pred, p_pred = model.predict(x_star, y_star, t_star)\n",
        "    lambda_1_value = model.sess.run(model.lambda_1)\n",
        "    lambda_2_value = model.sess.run(model.lambda_2)\n",
        "    \n",
        "    # Error\n",
        "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "    error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
        "    error_p = np.linalg.norm(p_star-p_pred,2)/np.linalg.norm(p_star,2)\n",
        "\n",
        "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
        "    error_lambda_2 = np.abs(lambda_2_value - 0.01)/0.01 * 100\n",
        "    print('Error u: %e' % (error_u))    \n",
        "    print('Error v: %e' % (error_v))    \n",
        "    print('Error p: %e' % (error_p))    \n",
        "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
        "    print('Error l2: %.5f%%' % (error_lambda_2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy_MELfvlHUG",
        "colab_type": "text"
      },
      "source": [
        "### Implement the plotting function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "p7hZUhOdlHUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_solution(X_star, u_star, index):\n",
        "    \n",
        "    lb = X_star.min(0)\n",
        "    ub = X_star.max(0)\n",
        "    nn = 200\n",
        "    x = np.linspace(lb[0], ub[0], nn)\n",
        "    y = np.linspace(lb[1], ub[1], nn)\n",
        "    X, Y = np.meshgrid(x,y)\n",
        "    \n",
        "    U_star = griddata(X_star, u_star.flatten(), (X, Y), method='cubic')\n",
        "    \n",
        "    plt.figure(index)\n",
        "    plt.pcolor(X,Y,U_star, cmap = 'jet')\n",
        "    plt.colorbar()\n",
        "    \n",
        "    \n",
        "def axisEqual3D(ax):\n",
        "    extents = np.array([getattr(ax, 'get_{}lim'.format(dim))() for dim in 'xyz'])\n",
        "    sz = extents[:,1] - extents[:,0]\n",
        "    centers = np.mean(extents, axis=1)\n",
        "    maxsize = max(abs(sz))\n",
        "    r = maxsize/4\n",
        "    for ctr, dim in zip(centers, 'xyz'):\n",
        "        getattr(ax, 'set_{}lim'.format(dim))(ctr - r, ctr + r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E57QTvLVlHUJ",
        "colab_type": "text"
      },
      "source": [
        "### Prediction plot of original test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yyc72tslHUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    plot_solution(X_star, u_star, 1)\n",
        "    plot_solution(X_star, v_star, 2)\n",
        "    plot_solution(X_star, p_star, 3)\n",
        "    plot_solution(X_star, p_star, 4)\n",
        "    plot_solution(X_star, p_star - p_pred, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf9dmJyXlHUM",
        "colab_type": "text"
      },
      "source": [
        "### Prediction plot of noiseless data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Z4-GgEfslHUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    plot_solution(X_star, u_pred, 1)\n",
        "    plot_solution(X_star, v_pred, 2)\n",
        "    plot_solution(X_star, p_pred, 3)\n",
        "    plot_solution(X_star, p_star, 4)\n",
        "    plot_solution(X_star, p_star - p_pred, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vSIUEjRlHUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    print(np.linalg.norm(p_star-p_pred,2)/np.linalg.norm(p_star,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC7sXLl5lHUU",
        "colab_type": "text"
      },
      "source": [
        "### Training stage for noisy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Dfs_hDZblHUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    noise = 0.01        \n",
        "    u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "    v_train = v_train + noise*np.std(v_train)*np.random.randn(v_train.shape[0], v_train.shape[1])    \n",
        "    # Training\n",
        "    model = PhysicsInformedNN(x_train, y_train, t_train, u_train, v_train, layers)\n",
        "    model.train(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63gKRFt6lHUW",
        "colab_type": "text"
      },
      "source": [
        "### Testing stage for noisy data  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gKdp44nPlHUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    lambda_1_value_noisy = model.sess.run(model.lambda_1)\n",
        "    lambda_2_value_noisy = model.sess.run(model.lambda_2)\n",
        "    u_pred_noisy, v_pred_noisy, p_pred_noisy = model.predict(x_star, y_star, t_star)\n",
        "    \n",
        "    \n",
        "    error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\n",
        "    error_lambda_2_noisy = np.abs(lambda_2_value_noisy - 0.01)/0.01 * 100\n",
        "    error_u_noisy = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "    error_v_noisy = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
        "    error_p_noisy = np.linalg.norm(p_star-p_pred,2)/np.linalg.norm(p_star,2)\n",
        "\n",
        "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
        "    error_lambda_2 = np.abs(lambda_2_value - 0.01)/0.01 * 100\n",
        "    print('Error u_noisy: %e' % (error_u_noisy))    \n",
        "    print('Error v_noisy: %e' % (error_v_noisy))    \n",
        "    print('Error p_noisy: %e' % (error_p_noisy))       \n",
        "    print('Error l1_noisy: %.5f%%' % (error_lambda_1_noisy))                             \n",
        "    print('Error l2_noisy: %.5f%%' % (error_lambda_2_noisy))     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmNV2b0dlHUZ",
        "colab_type": "text"
      },
      "source": [
        "### Prediction plot of noiseless prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fOXTr7hGlHUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    plot_solution(X_star, u_pred_noisy, 1)\n",
        "    plot_solution(X_star, v_pred_noisy, 2)\n",
        "    plot_solution(X_star, p_pred_noisy, 3)\n",
        "    plot_solution(X_star, p_star, 4)\n",
        "    plot_solution(X_star, p_star - p_pred_noisy, 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}